model: "models/Qwen3-0.6B" 
train: true 
fine_tune_type: lora
optimizer: adamw 
data: "mlx_data" 
seed: 0 
num_layers: 28 
batch_size: 1 
iters: 500  
val_batches: 25 
learning_rate: 1e-4 
project_name: MLX-FT-Qwen3  
steps_per_report: 10  
steps_per_eval: 200 
resume_adapter_file: null 
adapter_path: "cog_adapters"  
save_every: 100 
test: false 
test_batches: 100 
max_seq_length: 512 
grad_checkpoint: false 
lora_parameters:  
  keys: ["self_attn.q_proj", "self_attn.v_proj"]
  rank: 8
  scale: 20.0
  dropout: 0.0